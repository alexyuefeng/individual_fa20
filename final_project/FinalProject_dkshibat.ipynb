{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Final Project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project seeks to use the text from Yelp reviews on parks to predict what the corresponding Yelp rating would be on a scale of 1-5. It utilized TF-IDF and an SVC linear model to run this prediction. Ultimately, it ended up being a servicable predictor, achieving an accuracy of 52%, but one that also had much room for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name & GitHub\n",
    "\n",
    "- Name: Derek Shibata\n",
    "- GitHub Username: dkshibat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using text analysis from Yelp reviews, how accurately can we predict what rating a park will get on Yelp? Can we, using these metrics and word analysis, then construct a fictional park that would earn very high ratings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There has been lots of research and work done on Yelp in general, with it being one of the most used websites for reviews and ratings. However, most of these reviews have been on restaurants as opposed to parks. This is a significant difference as the words used to review restaurants can be substantially different than those used to describe parks.\n",
    "\n",
    "From these studies, they used a couple of different methods to do this prediction, including Neural networks and linear models. Overall, they were able to pretty accurately predict what rating a review would give, with it getting up to around 88% with the neural network. I don't expect to get this high of an accuracy, since the reviews on parks can range much more and I feel that the words used to describe a 3 star vs. a 5 star won't differ that much. In addition, I will stick with more simplistic models than using neural networks.\n",
    "\n",
    "References (include links):\n",
    "- 1) https://www.kaggle.com/athoul01/predicting-yelp-ratings-from-review-text\n",
    "- 2) https://www.researchgate.net/publication/280102160_Prediction_of_Yelp_Rating_using_Yelp_Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While I do think it is possible to predict Yelp ratings, I ultimately don't think it is feasible to construct a fictional park that would earn high ratings. I think that the text analysis will turn out to be just a sentiment analysis, with the key words it pulls from the reviews only conveying how the reviewer feels as opposed to them using words that will allow one to see what makes a good park."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dataset Name: Yelp Park Reviews\n",
    "- Link to the dataset: https://github.com/dkshibat/individual_fa20/blob/master/data/yelp_SD_reviews.csv\n",
    "- Number of observations: 2333\n",
    "\n",
    "This dataset contains reviews on Yelp for parks in San Diego. Each observation has an id, which is just the park name, a rating in a range of 1-5, and text, which is the review as a string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dkshi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, there was very little data cleaning that needed to be done for the dataset. I renamed the 'id' column to be called 'park' and 'text' to 'review' so that it was more descriptive. There were no missing values from this dataset, so nothing needed to be done there. I made sure that the rating section of the data was read in as an integer so that I could use a linear model. I also shuffled the data to ensure that there was a random assortment of parks in the training and test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>park</th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1311</th>\n",
       "      <td>Lora Haight</td>\n",
       "      <td>5</td>\n",
       "      <td>We were extremely satisfied with Lora as our r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>Ocean Beach Veteran's Memorial</td>\n",
       "      <td>4</td>\n",
       "      <td>The plaque at the Veteran's Memorial in Ocean ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>San Diego School of Survival</td>\n",
       "      <td>5</td>\n",
       "      <td>Working with this company was easy and afforda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>City of San Diego</td>\n",
       "      <td>5</td>\n",
       "      <td>So this was my first time hanging out in San D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1341</th>\n",
       "      <td>Torrey Pines Gliderport</td>\n",
       "      <td>5</td>\n",
       "      <td>What an awesome spot this is! My wife and I wa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                park  rating  \\\n",
       "1311                     Lora Haight       5   \n",
       "773   Ocean Beach Veteran's Memorial       4   \n",
       "513     San Diego School of Survival       5   \n",
       "499                City of San Diego       5   \n",
       "1341         Torrey Pines Gliderport       5   \n",
       "\n",
       "                                                 review  \n",
       "1311  We were extremely satisfied with Lora as our r...  \n",
       "773   The plaque at the Veteran's Memorial in Ocean ...  \n",
       "513   Working with this company was easy and afforda...  \n",
       "499   So this was my first time hanging out in San D...  \n",
       "1341  What an awesome spot this is! My wife and I wa...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/yelp_SD_reviews.csv', dtype={'rating':int})\n",
    "df = df.rename(columns = {'id' : 'park', 'text': 'review'})\n",
    "df = shuffle(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis & Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I wanted to see how many of each rating there were in the dataset. As I thought, there were the most 4 and 5 ratings. I expected there to be more 1s than there were, but these were still the third most common rating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x224e7b10c18>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.catplot(y='rating', kind='count',data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the Tfidf vectorizer as used in A6 to transform the reviews into an array. Then, I separated the data into a train and test set, which was on a 90/10 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(sublinear_tf=True,analyzer='word',max_features=2000,tokenizer=word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = tfidf.fit_transform(df['review']).toarray()\n",
    "train_df_X = df_X[:int(len(df_X)*.9)]\n",
    "test_df_X = df_X[int(len(df_X)*.9):]\n",
    "df_Y = df['rating']\n",
    "train_df_Y = df_Y[:int(len(df_Y)*.9)]\n",
    "test_df_Y = df_Y[int(len(df_Y)*.9):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried to train on a few different SVC kernels: linear, poly, and rbf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = SVC(kernel='linear')\n",
    "trainer.fit(train_df_X,train_df_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear model got the best results. As I expected, it was very hard for the predictor to predict ratings that would be in the range of 2-3. It got none of those correct on the test data. However, despite this, it still was able to achieve a weighted accuracy of .52, which is a solid improvement over the .2 accuracy you would get from random guesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.94      0.58      0.72       178\n",
      "           2       1.00      0.03      0.06        63\n",
      "           3       1.00      0.25      0.40       166\n",
      "           4       0.80      0.59      0.68       469\n",
      "           5       0.75      0.98      0.85      1223\n",
      "\n",
      "    accuracy                           0.77      2099\n",
      "   macro avg       0.90      0.49      0.54      2099\n",
      "weighted avg       0.81      0.77      0.74      2099\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.83      0.22      0.34        23\n",
      "           2       0.00      0.00      0.00         8\n",
      "           3       0.00      0.00      0.00        20\n",
      "           4       0.30      0.19      0.23        42\n",
      "           5       0.64      0.91      0.75       141\n",
      "\n",
      "    accuracy                           0.61       234\n",
      "   macro avg       0.35      0.26      0.27       234\n",
      "weighted avg       0.52      0.61      0.53       234\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dkshi\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "pred_train_df_Y = trainer.predict(train_df_X)\n",
    "pred_test_df_Y = trainer.predict(test_df_X)\n",
    "print(classification_report(train_df_Y,pred_train_df_Y))\n",
    "print(classification_report(test_df_Y, pred_test_df_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dkshi\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='poly', max_iter=-1, probability=False, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = SVC(kernel='poly')\n",
    "trainer.fit(train_df_X,train_df_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00       178\n",
      "           2       0.00      0.00      0.00        63\n",
      "           3       0.00      0.00      0.00       166\n",
      "           4       0.00      0.00      0.00       469\n",
      "           5       0.58      1.00      0.74      1223\n",
      "\n",
      "    accuracy                           0.58      2099\n",
      "   macro avg       0.12      0.20      0.15      2099\n",
      "weighted avg       0.34      0.58      0.43      2099\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00        23\n",
      "           2       0.00      0.00      0.00         8\n",
      "           3       0.00      0.00      0.00        20\n",
      "           4       0.00      0.00      0.00        42\n",
      "           5       0.60      1.00      0.75       141\n",
      "\n",
      "    accuracy                           0.60       234\n",
      "   macro avg       0.12      0.20      0.15       234\n",
      "weighted avg       0.36      0.60      0.45       234\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dkshi\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "pred_train_df_Y = trainer.predict(train_df_X)\n",
    "pred_test_df_Y = trainer.predict(test_df_X)\n",
    "print(classification_report(train_df_Y,pred_train_df_Y))\n",
    "print(classification_report(test_df_Y, pred_test_df_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dkshi\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = SVC(kernel='rbf')\n",
    "trainer.fit(train_df_X,train_df_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00       178\n",
      "           2       0.00      0.00      0.00        63\n",
      "           3       0.00      0.00      0.00       166\n",
      "           4       0.00      0.00      0.00       469\n",
      "           5       0.58      1.00      0.74      1223\n",
      "\n",
      "    accuracy                           0.58      2099\n",
      "   macro avg       0.12      0.20      0.15      2099\n",
      "weighted avg       0.34      0.58      0.43      2099\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00        23\n",
      "           2       0.00      0.00      0.00         8\n",
      "           3       0.00      0.00      0.00        20\n",
      "           4       0.00      0.00      0.00        42\n",
      "           5       0.60      1.00      0.75       141\n",
      "\n",
      "    accuracy                           0.60       234\n",
      "   macro avg       0.12      0.20      0.15       234\n",
      "weighted avg       0.36      0.60      0.45       234\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dkshi\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "pred_train_df_Y = trainer.predict(train_df_X)\n",
    "pred_test_df_Y = trainer.predict(test_df_X)\n",
    "print(classification_report(train_df_Y,pred_train_df_Y))\n",
    "print(classification_report(test_df_Y, pred_test_df_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main concern regarding privacy is that no personal information was revealed in the reviews themself. For instance, someone could have possibly mentioned the names of relatives or friends, or stated that the park is next to their house. If the authors of the reviews are public, then this will definitely need to be removed from the data as part of data cleaning, as this is in direct violation of privacy. After reading the Yelp Dataset Terms of Use, I am allowed to use the data provided by Yelp for academic studies.\n",
    "\n",
    "The main bias I can think of is the fact that most reviewers come from people who think very highly of a park or had a horrible experience with it, with a significant lack of reviews from people who found it to be just mediocre or acceptable. Because of this, it may be hard to accurately predict park ratings on a scale of 1-5 when most are either 1 or 4/5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion & Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project was quite interesting to do and didn't prove to be too difficult. The amount of data given could have been greater, which would understandably have made the model more accurate. However, with the relatively small amount of available data, it was able to correctly predict the rating based on the review contents over half of the time. While this is not at the level of the 88% accuracy seen in the studies mentioned in the background, it is servicable for a simple linear model.\n",
    "\n",
    "This study goes to show that the words used in reviews, even for parks, have a lot of similarities between each other to the point where linear models can make accurate predictions about how the reviewer would rate the said park. The English language can be used in a way where words alone are sufficient to make predictions that are in a way categorical. However, this still turned out to be more of a sentiment analysis, with the positives correlating to 4-5 star reviews while the negative would be 1 star. In the future, more complex models would have to be used to fit categorical data such as this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
